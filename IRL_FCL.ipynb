{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43c7d69-ee78-4ea5-964d-991579f85ffc",
   "metadata": {},
   "source": [
    "## Inverse Reinforcement Learning for Financial Cliff Walking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbcbc0-99b8-45b3-98c0-995caeaedc1e",
   "metadata": {},
   "source": [
    "This notebook contains implementations of three IRL algorithms for the Financial Cliff Walking (FCW) problem:\n",
    "\n",
    "Max Causal Entropy IRL\n",
    "\n",
    "IRL from Failure (IRLF)\n",
    "\n",
    "T-REX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4268db9a-f6c9-4185-b44c-73aed25aa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4525113-1e4d-49f9-9741-5ab02d887e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# N - World height\n",
    "# T - World width\n",
    "WORLD_HEIGHT = 4\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "# Probability for exploration - epsilon\n",
    "EPSILON = 0.1\n",
    "# Step size\n",
    "ALPHA = 0.001\n",
    "# Gamma - discount factor - for Q-Learning, Sarsa and Expected Sarsa\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Actions - ACTION_UP is a+ (adding a deposit), ACTION_DOWN is a-(redeeming a deposit) and \n",
    "# ACTION_ZERO is a0 (leaving the account as it is).\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_ZERO = 2\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_ZERO]\n",
    "\n",
    "# Initial and Goal states\n",
    "START = [1,0]\n",
    "GOAL = [0, WORLD_WIDTH-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bd452-2598-46d2-8de3-52b43f6c86c1",
   "metadata": {},
   "source": [
    "### Functions determining the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b98f80-3dd2-4dec-b032-cf130de67354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step function that describes how the next state is obtained from the current state and the action \n",
    "# taken. The function returns the next state and the reward obtained.\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "\n",
    "    if state[0] == 0 and (state[1] > 0): #  and state[1] < WORLD_WIDTH - 2):\n",
    "        # remain in the bankruptcy state\n",
    "        next_state =  [0, min(j + 1, WORLD_WIDTH - 1)]\n",
    "        reward = 0 \n",
    "        return next_state, reward\n",
    "    \n",
    "    # if at the final time, next state is the same, and reward is zero\n",
    "    if state[1] == WORLD_WIDTH - 1:\n",
    "        next_state = [i,state[1]]\n",
    "        reward = 0\n",
    "        return next_state, reward\n",
    "    \n",
    "    if action == ACTION_UP:\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT-1), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_DOWN:\n",
    "        next_state = [max(i - 1, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_ZERO:\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH - 1)]\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    # The reward is -1 for actions ACTION_UP and ACTION_DOWN. This is done to keep transactions to a minimum.\n",
    "    reward = -1\n",
    "    \n",
    "    # ACTION_ZERO gets a zero reward since we want to minimize the number of transactions\n",
    "    if action == ACTION_ZERO:\n",
    "        reward = 0\n",
    "    \n",
    "    # Exceptions are \n",
    "    # i) If bankruptcy happens before WORLD_WIDTH time steps\n",
    "    # ii) No deposit at initial state\n",
    "    # iii) Redemption at initial state!\n",
    "    # iv) Any action carried out from a bankrupt state\n",
    "    if ((action == ACTION_DOWN and i == 1 and 1 <= j < 10) or (\n",
    "        action == ACTION_ZERO and state == START) or (\n",
    "        action == ACTION_DOWN and state == START )) or (\n",
    "        i == 0 and 1 <= j <= 10):    \n",
    "            reward = -100\n",
    "        \n",
    "    # Next exception is when we get to the final time step.\n",
    "    if state[0] != 0 and (next_state[1] == WORLD_WIDTH - 1): \n",
    "        # override a random action by a deterministic action=ACTION_DOWN\n",
    "        if (next_state[0] == 0): # Action resulted in ending with zero balance in final time step\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -10   \n",
    "        \n",
    "    return next_state, reward\n",
    "\n",
    "# Choose an action based on epsilon greedy algorithm\n",
    "def choose_action(state, q_value, eps=EPSILON):\n",
    "    if np.random.binomial(1, eps) == 1:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice([action_ for action_, value_ in enumerate(values_) \n",
    "                                 if value_ == np.max(values_)])\n",
    "    # From bankrupt state there is no meaningful action, so we will assign 'Z' by convention.\n",
    "    if state[0] == 0 and state[1] > 0:\n",
    "        action = ACTION_ZERO\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "965748f0-9266-47e2-abeb-fd06df04ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos2idx(pos):\n",
    "    \"\"\"\n",
    "    input:\n",
    "      column-major 2d position\n",
    "    returns:\n",
    "      1d index\n",
    "    \"\"\"\n",
    "    return np.ravel_multi_index((pos[0], pos[1]), (WORLD_HEIGHT, WORLD_WIDTH))\n",
    "\n",
    "def idx2pos(idx):\n",
    "        \n",
    "    \"\"\"\n",
    "    input:\n",
    "      1d idx\n",
    "    returns:\n",
    "      2d column-major position\n",
    "    \"\"\"\n",
    "    \n",
    "    pos = [np.unravel_index(idx, (WORLD_HEIGHT, WORLD_WIDTH))[0],\n",
    "           np.unravel_index(idx, (WORLD_HEIGHT, WORLD_WIDTH))[1]]\n",
    "    return pos \n",
    "\n",
    "\n",
    "def get_transition_mat():\n",
    "    \n",
    "    \"\"\"\n",
    "    get transition dynamics of the gridworld\n",
    "\n",
    "    return:\n",
    "      P_a         NxNxN_ACTIONS transition probabilities matrix - \n",
    "                    P_a[s0, s1, a] is the transition prob of \n",
    "                    landing at state s1 when taking action \n",
    "                    a at state s0\n",
    "                    \n",
    "    Probabilities are ones for transitions to states produced by function step(state,action)\n",
    "    \"\"\"\n",
    "    n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "    n_actions = len(ACTIONS)\n",
    "    \n",
    "    P_a = np.zeros((n_states, n_states, n_actions))\n",
    "    \n",
    "    for si in range(1, n_states): \n",
    "        for a in ACTIONS:            \n",
    "            next_state, reward = step(idx2pos(si),a)\n",
    "            sj = pos2idx(next_state)                         \n",
    "            P_a[int(si), int(sj), int(a)] = 1\n",
    "\n",
    "    return P_a\n",
    "\n",
    "def get_true_rewards():\n",
    "    \"\"\"\n",
    "    get the reward matrix of shape n_states x n_actions\n",
    "    \"\"\"\n",
    "    n_states = WORLD_HEIGHT * WORLD_WIDTH\n",
    "    n_actions = len(ACTIONS)\n",
    "    \n",
    "    gt_rewards = np.zeros((n_states, n_actions))\n",
    "    for si in range(n_states):\n",
    "        for a in ACTIONS:            \n",
    "            next_state, reward = step(idx2pos(si),a)                        \n",
    "            gt_rewards[int(si), int(a)] = reward\n",
    "    \n",
    "    return gt_rewards\n",
    "    \n",
    "def get_rewards_on_rectangle(rewards, action): # matrix of size n_state x n_action\n",
    "                            \n",
    "    # convert rewards into rectangular form WORLD_HEIGHT x WORLD_WIDTH\n",
    "    rewards_rectangle = np.zeros((WORLD_HEIGHT, WORLD_WIDTH))\n",
    "    n_states = rewards.shape[0]\n",
    "    \n",
    "    for idx in range(n_states):\n",
    "        height, width = idx2pos(idx)\n",
    "        rewards_rectangle[height,width] = rewards[idx,action]\n",
    "    \n",
    "    return rewards_rectangle\n",
    "\n",
    "def get_values_on_rectangle(values): # here values are given by the vector of size n_states\n",
    "    \n",
    "    values_rectangle = np.zeros((WORLD_HEIGHT, WORLD_WIDTH))\n",
    "    n_states = len(values)\n",
    "    \n",
    "    for idx in range(n_states):\n",
    "        height, width = idx2pos(idx)\n",
    "        values_rectangle[height,width] = values[idx]\n",
    "    \n",
    "    return values_rectangle\n",
    "    \n",
    "def value_iteration(P_a, rewards, gamma, error=0.01, deterministic=False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    static value iteration function\n",
    "  \n",
    "    inputs:\n",
    "    P_a         N x N x N_ACTIONS transition probabilities matrix - \n",
    "                              P_a[s0, s1, a] is the transition prob of \n",
    "                              landing at state s1 when taking action \n",
    "                              a at state s0\n",
    "    rewards     N x N_ACTIONS matrix - rewards for all the state-action combinations\n",
    "    gamma       float - RL discount\n",
    "    error       float - threshold for a stop\n",
    "    deterministic   bool - to return deterministic policy or stochastic policy\n",
    "  \n",
    "    returns:\n",
    "    values    Nx1 matrix - estimated values\n",
    "    policy    Nx1 (NxN_ACTIONS if non-det) matrix - policy\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    values = np.zeros([n_states])\n",
    "\n",
    "    # estimate values\n",
    "    while True:\n",
    "        values_tmp = values.copy()\n",
    "\n",
    "        for s in range(n_states):\n",
    "            v_s = []\n",
    "            values[s] = max([sum([P_a[s, s1, a]*(rewards[s,a] + gamma*values_tmp[s1]) for s1 in range(n_states)]) \n",
    "                             for a in ACTIONS])    \n",
    "        \n",
    "        if max([abs(values[s] - values_tmp[s]) for s in range(n_states)]) < error:\n",
    "            break\n",
    "\n",
    "\n",
    "    if deterministic:\n",
    "        # generate deterministic policy\n",
    "        policy = np.zeros([n_states])\n",
    "        for s in range(n_states):\n",
    "            policy[s] = np.argmax([sum([P_a[s, s1, a]*(rewards[s,a]+gamma*values[s1]) \n",
    "                                  for s1 in range(n_states)]) \n",
    "                                  for a in ACTIONS])\n",
    "\n",
    "        return values, policy\n",
    "    else:\n",
    "        # generate stochastic policy\n",
    "        policy = np.zeros([n_states, n_actions])\n",
    "        for s in range(n_states):\n",
    "            v_s = np.array([sum([P_a[s, s1, a]*(rewards[s,a] + gamma*values[s1]) for s1 in range(n_states)]) \n",
    "                            for a in ACTIONS])\n",
    "            policy[s,:] = np.transpose(v_s/np.sum(v_s))\n",
    "        return values, policy\n",
    "    \n",
    "    \n",
    "def soft_Q_iteration(rewards, gamma, beta, learn_rate_Q, n_states, n_actions, \n",
    "                     trajs, error=0.01, max_iter = 500):\n",
    "    \"\"\"\n",
    "    Performs soft Q-iteration based on observed samples \n",
    "    Returns: the soft-Q function, soft-V function, and soft-Q policy\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    T = trajs.shape[1] \n",
    "    n_traj = trajs.shape[0]\n",
    "    \n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "        \n",
    "    # initialize\n",
    "    soft_Q_vals =  np.zeros((n_states, n_actions))\n",
    "    soft_Q_policy = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    iter_idx = 0\n",
    "    \n",
    "    # soft-Q iteration\n",
    "    while True:\n",
    "        soft_Q_vals_tmp = soft_Q_vals.copy()\n",
    "        \n",
    "        for episode in range(n_traj):\n",
    "            for t in range(T):\n",
    "                state = trajs[episode,t,0]\n",
    "                action = trajs[episode,t,1]\n",
    "                next_state = trajs[episode,t,2]\n",
    "                reward = rewards[state, action]\n",
    "                \n",
    "                soft_V_vals_next = (1.0/beta) * np.log(np.sum(np.exp(beta*soft_Q_vals[next_state,:])))\n",
    "                soft_Q_vals[state, action] += learn_rate_Q*(reward + gamma * soft_V_vals_next \n",
    "                                                            - soft_Q_vals[state, action])\n",
    "        \n",
    "        iter_idx += 1\n",
    "        if np.max(soft_Q_vals - soft_Q_vals_tmp) < error or iter_idx >= max_iter:\n",
    "            break\n",
    "\n",
    "    # soft V-function        \n",
    "    soft_V_vals = (1.0/beta) * np.log(np.sum(np.exp(beta*soft_Q_vals),axis=1))\n",
    "    \n",
    "    # stochastic policy\n",
    "    policy = np.exp(beta*(soft_Q_vals - np.tile(soft_V_vals[:, np.newaxis],(1,n_actions) )))\n",
    "            \n",
    "    return soft_Q_vals, soft_V_vals, policy\n",
    "\n",
    "\n",
    "def soft_Q_iteration_mb(P_a, rewards, gamma, beta, n_states, n_actions, error=0.01):\n",
    "    \"\"\"\n",
    "    Soft Q-iteration based on a transition model (model-based soft Q-iteration)\n",
    "    \"\"\"\n",
    "    \n",
    "    Q_values = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # estimate Q-values\n",
    "    while True:\n",
    "        Q_values_tmp = Q_values.copy()\n",
    "\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):   \n",
    "                Q_values[s,a] = np.sum(P_a[s,:,a]*(rewards[s,a]\n",
    "                                        + (gamma/beta)*np.log(np.sum(np.exp(beta*Q_values_tmp[:,:]),axis=1))))\n",
    "        if np.max(np.abs(Q_values - Q_values_tmp)) < error:\n",
    "            break\n",
    "\n",
    "    # soft V-function        \n",
    "    soft_V_vals = (1.0/beta) * np.log(np.sum(np.exp(beta*Q_values),axis=1))\n",
    "    \n",
    "    # stochastic policy\n",
    "    policy = np.exp(beta*(Q_values - np.tile(soft_V_vals[:, np.newaxis],(1,n_actions) )))\n",
    "            \n",
    "    return Q_values, soft_V_vals, policy            \n",
    "        \n",
    "def compute_state_visition_freq(P_a, gamma, T, policy, deterministic=False):\n",
    "    \n",
    "    \"\"\"compute the expected states visition frequency p(s| theta, T) \n",
    "    using dynamic programming\n",
    "\n",
    "    inputs:\n",
    "    P_a     N x N x N_ACTIONS matrix - transition dynamics\n",
    "    gamma   float - discount factor\n",
    "    trajs   list of list of Steps - collected from expert\n",
    "    policy  N x 1 vector (or NxN_ACTIONS if deterministic=False) - policy\n",
    "    T       number of time steps\n",
    "  \n",
    "    returns:\n",
    "    svf       N x 1 vector - state visitation frequencies\n",
    "    savf      N x N_ACTION - state-action visitation frequencies\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "    \n",
    "    # mu[s, t] is the prob of visiting state s at time t\n",
    "    mu = np.zeros([n_states, T]) \n",
    "    \n",
    "    # as all trajectories in our case start at [0,0], initialize the time-dependent state density at t = 0\n",
    "    mu[pos2idx(START),0] = 1\n",
    "\n",
    "    # compute mu[s,t] for other times \n",
    "    for s in range(n_states):\n",
    "        for t in range(T-1):\n",
    "            if deterministic:\n",
    "                mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] \n",
    "                                  for pre_s in range(n_states)])\n",
    "            else:\n",
    "                mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] \n",
    "                                       for a1 in ACTIONS]) for pre_s in range(n_states)])\n",
    "\n",
    "    \n",
    "    # array of discount factors\n",
    "    discounts = np.ones(T)\n",
    "    for i in range(1,T):\n",
    "        discounts[i] = gamma * discounts[i-1]\n",
    "    \n",
    "    # state visitation frequencies\n",
    "    svf = np.sum(mu * discounts, 1)\n",
    "        \n",
    "    # state-action visitation frequencies\n",
    "    savf = np.tile(svf[:,np.newaxis],(1,n_actions)) * policy\n",
    "    \n",
    "    return svf, savf\n",
    "\n",
    "\n",
    "def maxent_irl(feat_map, P_a, gamma, beta, lr, lr_Q, trajs, n_iters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "\n",
    "    inputs:\n",
    "        feat_map    NxD matrix - the features for each state\n",
    "        P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                           landing at state s1 when taking action \n",
    "                                           a at state s0\n",
    "        gamma       float - RL discount factor\n",
    "        beta        regularization parameter for soft Q-learning\n",
    "        lr          learning rate\n",
    "        lr_Q        learning rate for soft Q-learning\n",
    "        trajs       a list of demonstrations\n",
    "        \n",
    "        n_iters     int - number of optimization steps\n",
    "\n",
    "    returns\n",
    "        rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    \n",
    "    # theta is a matrix of parameters of shape n_states x n_actions, for actions = 0,1,2\n",
    "    theta = np.random.uniform(low=-0.1,high=0.1, size=(feature_matrix.shape[1],n_actions))    \n",
    "\n",
    "    # calc feature expectations\n",
    "    feat_exp = np.zeros((feat_map.shape[1],n_actions))\n",
    "    \n",
    "    T = trajs.shape[1]\n",
    "    n_traj = trajs.shape[0]\n",
    "    \n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "    \n",
    "    for episode in range(n_traj):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs[episode,t,0]\n",
    "                action = trajs[episode,t,1]\n",
    "                feat_exp[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp = feat_exp/n_traj \n",
    "    \n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "  \n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "    \n",
    "        # compute reward function as a matrix of shape n_states x n_actions, for actions = 0,1,2\n",
    "        rewards = np.dot(feat_map, theta)\n",
    "        \n",
    "        # try computing policy using model-based soft Q-iteration\n",
    "        _, _, policy = soft_Q_iteration_mb(P_a, rewards, gamma, beta, \n",
    "                                           n_states, n_actions, error=0.01)\n",
    "    \n",
    "        # compute state-action visition frequences\n",
    "        _, savf = compute_state_visition_freq(P_a, gamma, T, policy, deterministic=False)\n",
    "            \n",
    "        # compute gradients\n",
    "        grad = feat_exp - feat_map.T.dot(savf) # now gradient has shape n_states x n_actions\n",
    "\n",
    "        # update params\n",
    "        theta += lr * grad\n",
    "\n",
    "    rewards = np.dot(feat_map, theta)\n",
    "    return rewards \n",
    "\n",
    "# use this function from irl-imitation for displaying results\n",
    "def heatmap2d(hm_mat, title='', block=True, fig_num=1, text=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Display heatmap\n",
    "    input:\n",
    "        hm_mat:   mxn 2d np array\n",
    "    \"\"\"\n",
    "    print ('map shape: {}, data type: {}'.format(hm_mat.shape, hm_mat.dtype))\n",
    "\n",
    "    if block:\n",
    "        plt.figure(fig_num)\n",
    "        plt.clf()\n",
    "  \n",
    "    plt.imshow(hm_mat, interpolation='nearest',origin=\"lower\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "  \n",
    "    if text:\n",
    "        for y in range(hm_mat.shape[0]):\n",
    "            for x in range(hm_mat.shape[1]):\n",
    "                plt.text(x, y, '%.1f' % hm_mat[y, x],\n",
    "                     horizontalalignment='center',\n",
    "                     verticalalignment='center',\n",
    "                     )\n",
    "\n",
    "    if block:\n",
    "        plt.ion()\n",
    "        print ('press enter to continue')\n",
    "        plt.show()\n",
    "        raw_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b4da9-11db-4b33-82bf-9608f2fd0d3b",
   "metadata": {},
   "source": [
    "### IRLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abec5427-196d-4f30-a174-6cf453df456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_from_failure(feat_map, P_a, gamma, \n",
    "                     beta,\n",
    "                     trajs_success, \n",
    "                     trajs_failure,\n",
    "                     lr,\n",
    "                     lr_Q,\n",
    "                     alpha_lam,\n",
    "                     lam,\n",
    "                     lam_min, \n",
    "                     n_iters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)\n",
    "\n",
    "    inputs:\n",
    "        feat_map    NxD matrix - the features for each state\n",
    "        P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of \n",
    "                                           landing at state s1 when taking action \n",
    "                                           a at state s0\n",
    "        gamma               float - RL discount factor\n",
    "        trajs_success       successful demonstrations\n",
    "        trajs_failure       failed demonstrations\n",
    "        lr          float - learning rate\n",
    "        n_iters     int - number of optimization steps\n",
    "\n",
    "    returns\n",
    "        rewards     Nx1 vector - recoverred state rewards\n",
    "    \"\"\"\n",
    "    n_states, _, n_actions = np.shape(P_a)\n",
    "\n",
    "    # init parameters\n",
    "    \n",
    "    # theta_s and theta_f are sets of parameters for successful and failed trajectories\n",
    "    \n",
    "    # theta_s is a matrix of parameters of shape n_states x n_actions, for actions = 0,1,2\n",
    "    theta_s = np.random.uniform(low=-0.1,high=0.1, size=(feature_matrix.shape[1],n_actions))    \n",
    "    theta_f = np.zeros((feature_matrix.shape[1], n_actions))\n",
    "    \n",
    "    # calc feature expectations\n",
    "    feat_exp_s = np.zeros((feat_map.shape[1],n_actions))\n",
    "    feat_exp_f = np.zeros((feat_map.shape[1],n_actions))\n",
    "    \n",
    "    T_s = trajs_success.shape[1]\n",
    "    n_traj_s = trajs_success.shape[0]\n",
    "    T_f = trajs_failure.shape[1] \n",
    "    n_traj_f = trajs_failure.shape[0]\n",
    "    \n",
    "    # discount factors\n",
    "    T = T_s\n",
    "    discounts = np.ones(T)\n",
    "    for t in range(1,T):\n",
    "        discounts[t] *= gamma\n",
    "    \n",
    "    # empirical feature expectations for successful trajectories\n",
    "    for episode in range(n_traj_s):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs_success[episode,t,0]\n",
    "                action = trajs_success[episode,t,1]\n",
    "                feat_exp_s[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp_s = feat_exp_s/n_traj_s \n",
    "    \n",
    "    # empirical feature expectations for failed trajectories\n",
    "    for episode in range(n_traj_f):\n",
    "        for t in range(T):\n",
    "            for a in ACTIONS:\n",
    "                state = trajs_failure[episode,t,0]\n",
    "                action = trajs_failure[episode,t,1]\n",
    "                feat_exp_f[state, action] += discounts[t]\n",
    "    \n",
    "    feat_exp_f = feat_exp_f/n_traj_f \n",
    "    \n",
    "    # training\n",
    "    for iteration in range(n_iters):\n",
    "  \n",
    "        if iteration % (n_iters/20) == 0:\n",
    "            print('iteration: {}/{}'.format(iteration, n_iters))\n",
    "    \n",
    "        # compute reward function as a matrix of shape n_states x n_actions, for actions = 0,1,2\n",
    "        rewards = np.dot(feat_map, theta_s + theta_f)\n",
    "\n",
    "        # compute policy (using only successful trajectories?)\n",
    "        \n",
    "        # try computing policy using model-based soft Q-iteration\n",
    "        _, _, policy = soft_Q_iteration_mb(P_a, rewards, gamma, beta, \n",
    "                                           n_states, n_actions, error=0.01)\n",
    "    \n",
    "        # compute state-action visition frequency\n",
    "        _, savf_s = compute_state_visition_freq(P_a, gamma, T_s, policy, deterministic=False)\n",
    "            \n",
    "        # as both successful and failed trajectories start with the same initial state and \n",
    "        # have the same length, the state-action visitation frequency for failed trajectories is the same\n",
    "        savf_f = savf_s.copy()\n",
    "            \n",
    "        # compute gradients\n",
    "        grad_theta_s = feat_exp_s - feat_map.T.dot(savf_s) # now gradient has shape n_states x n_actions\n",
    "\n",
    "        # update params\n",
    "        theta_s += lr * grad_theta_s\n",
    "        \n",
    "        # recompute weights theta_f\n",
    "        theta_f = (savf_f - feat_exp_f)/lam\n",
    "        \n",
    "        if lam > lam_min:\n",
    "            lam = alpha_lam * lam\n",
    "\n",
    "    return rewards, policy, theta_s, theta_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b83cbf-f1e9-4b4e-9dd2-b26b05949e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
