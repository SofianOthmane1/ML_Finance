{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43c7d69-ee78-4ea5-964d-991579f85ffc",
   "metadata": {},
   "source": [
    "## Inverse Reinforcement Learning for Financial Cliff Walking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbcbc0-99b8-45b3-98c0-995caeaedc1e",
   "metadata": {},
   "source": [
    "This notebook contains implementations of three IRL algorithms for the Financial Cliff Walking (FCW) problem:\n",
    "\n",
    "Max Causal Entropy IRL\n",
    "\n",
    "IRL from Failure (IRLF)\n",
    "\n",
    "T-REX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4268db9a-f6c9-4185-b44c-73aed25aa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4525113-1e4d-49f9-9741-5ab02d887e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# N - World height\n",
    "# T - World width\n",
    "WORLD_HEIGHT = 4\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "# Probability for exploration - epsilon\n",
    "EPSILON = 0.1\n",
    "# Step size\n",
    "ALPHA = 0.001\n",
    "# Gamma - discount factor - for Q-Learning, Sarsa and Expected Sarsa\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Actions - ACTION_UP is a+ (adding a deposit), ACTION_DOWN is a-(redeeming a deposit) and \n",
    "# ACTION_ZERO is a0 (leaving the account as it is).\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_ZERO = 2\n",
    "ACTIONS = [ACTION_UP, ACTION_DOWN, ACTION_ZERO]\n",
    "\n",
    "# Initial and Goal states\n",
    "START = [1,0]\n",
    "GOAL = [0, WORLD_WIDTH-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bd452-2598-46d2-8de3-52b43f6c86c1",
   "metadata": {},
   "source": [
    "### Functions determining the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b98f80-3dd2-4dec-b032-cf130de67354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step function that describes how the next state is obtained from the current state and the action \n",
    "# taken. The function returns the next state and the reward obtained.\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "\n",
    "    if state[0] == 0 and (state[1] > 0): #  and state[1] < WORLD_WIDTH - 2):\n",
    "        # remain in the bankruptcy state\n",
    "        next_state =  [0, min(j + 1, WORLD_WIDTH - 1)]\n",
    "        reward = 0 \n",
    "        return next_state, reward\n",
    "    \n",
    "    # if at the final time, next state is the same, and reward is zero\n",
    "    if state[1] == WORLD_WIDTH - 1:\n",
    "        next_state = [i,state[1]]\n",
    "        reward = 0\n",
    "        return next_state, reward\n",
    "    \n",
    "    if action == ACTION_UP:\n",
    "        next_state = [min(i + 1, WORLD_HEIGHT-1), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_DOWN:\n",
    "        next_state = [max(i - 1, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
    "    elif action == ACTION_ZERO:\n",
    "        next_state = [i, min(j + 1, WORLD_WIDTH - 1)]\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    # The reward is -1 for actions ACTION_UP and ACTION_DOWN. This is done to keep transactions to a minimum.\n",
    "    reward = -1\n",
    "    \n",
    "    # ACTION_ZERO gets a zero reward since we want to minimize the number of transactions\n",
    "    if action == ACTION_ZERO:\n",
    "        reward = 0\n",
    "    \n",
    "    # Exceptions are \n",
    "    # i) If bankruptcy happens before WORLD_WIDTH time steps\n",
    "    # ii) No deposit at initial state\n",
    "    # iii) Redemption at initial state!\n",
    "    # iv) Any action carried out from a bankrupt state\n",
    "    if ((action == ACTION_DOWN and i == 1 and 1 <= j < 10) or (\n",
    "        action == ACTION_ZERO and state == START) or (\n",
    "        action == ACTION_DOWN and state == START )) or (\n",
    "        i == 0 and 1 <= j <= 10):    \n",
    "            reward = -100\n",
    "        \n",
    "    # Next exception is when we get to the final time step.\n",
    "    if state[0] != 0 and (next_state[1] == WORLD_WIDTH - 1): \n",
    "        # override a random action by a deterministic action=ACTION_DOWN\n",
    "        if (next_state[0] == 0): # Action resulted in ending with zero balance in final time step\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -10   \n",
    "        \n",
    "    return next_state, reward\n",
    "\n",
    "# Choose an action based on epsilon greedy algorithm\n",
    "def choose_action(state, q_value, eps=EPSILON):\n",
    "    if np.random.binomial(1, eps) == 1:\n",
    "        action = np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        values_ = q_value[state[0], state[1], :]\n",
    "        action = np.random.choice([action_ for action_, value_ in enumerate(values_) \n",
    "                                 if value_ == np.max(values_)])\n",
    "    # From bankrupt state there is no meaningful action, so we will assign 'Z' by convention.\n",
    "    if state[0] == 0 and state[1] > 0:\n",
    "        action = ACTION_ZERO\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965748f0-9266-47e2-abeb-fd06df04ed65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
